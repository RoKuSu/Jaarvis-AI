{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n",
      "-100.99613904012061\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import numpy\n",
    "import sklearn\n",
    "import dateutil\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import platform\n",
    "import os\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class DemandTrain:\n",
    "    def __init__(self):\n",
    "        # try:\n",
    "        #     if platform.system() == 'Windows':\n",
    "        self.Sql_connection1 = self.create_connection('/opt/apps/scripts/jaarvis_demand_supply/evo.db')\n",
    "\n",
    "        #     elif platform.system() == 'Linux':\n",
    "        #         self.Sql_connection1 = self.create_connection('/opt/apps/scripts/jaarvis_demand_supply/evo.db')\n",
    "        # except Error as e:\n",
    "        #     print(e)\n",
    "\n",
    "        self.df = pd.read_sql_query(\"select * from zones;\", self.Sql_connection1)\n",
    "\n",
    "    def create_connection(self, db_file):\n",
    "        \"\"\" create a database connection to the SQLite database\n",
    "            specified by the db_file\n",
    "        :param db_file: database file\n",
    "        :return: Connection object or None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(db_file)\n",
    "            return conn\n",
    "        except Error as e:\n",
    "            print(e)\n",
    "        return None\n",
    "\n",
    "    def data_cleansing(self):\n",
    "        self.df = self.df.dropna(how='any')\n",
    "        self.df.time = self.df.time.astype(str).str[:2].astype(int)\n",
    "        self.df.time = self.df.time.replace(regex=True, inplace=True, to_replace=r'\\D', value=r'')\n",
    "        self.df.time = [(x or -1) for x in self.df.time]\n",
    "        self.df.zone = [(x or 'unknown') for x in self.df.zone]\n",
    "        self.df.year = [(x or 0) for x in self.df.year]\n",
    "        self.df.month = [(x or 0) for x in self.df.month]\n",
    "        self.df.week = [(x or 0) for x in self.df.week]\n",
    "        self.df.day = [(x or 0) for x in self.df.day]\n",
    "        self.df.booked_vehicles = [(x or 0) for x in self.df.booked_vehicles]\n",
    "        self.df.free_vehicles = [(x or 0) for x in self.df.free_vehicles]\n",
    "\n",
    "        self.df = self.df.mask(self.df.eq('None')).dropna()\n",
    "        self.df = self.df.fillna(0)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def feature_addition(self):\n",
    "        # no feature added\n",
    "        pass\n",
    "\n",
    "    def feature_elimination(self):\n",
    "        # no feature deleted\n",
    "        pass\n",
    "\n",
    "    def training_data(self):\n",
    "\n",
    "        self.data_cleansing()\n",
    "        \n",
    "        df=self.df\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le = le.fit(df.zone)\n",
    "        numpy.save('classes.npy', le.classes_)        \n",
    "        pd.DataFrame({'zone': df['zone'].unique()}).to_csv(\"zones_list.csv\")\n",
    "        df['zone'] = le.transform(df.zone.get_values())\n",
    "        X = df[['zone','year', 'month', 'day', 'time']]       \n",
    "#         X = df[['zone', 'day', 'time']]\n",
    "        X = X.values\n",
    "        sc_X = StandardScaler()\n",
    "        X = sc_X.fit_transform(X)\n",
    "                \n",
    "        joblib.dump(sc_X,\"scaler.save\") \n",
    "\n",
    "        demand = df.booked_vehicles.values\n",
    "        free_vehicles = df.free_vehicles.values\n",
    "\n",
    "        return X, demand, free_vehicles\n",
    "\n",
    "    def trainRF(self):\n",
    "        X, demand, free_vehicles = self.training_data()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, demand, train_size=0.75, test_size=0.25)\n",
    "        tpot = TPOTRegressor(generations=100, population_size=100,\n",
    "                         offspring_size=None, mutation_rate=0.9,\n",
    "                         crossover_rate=0.1,\n",
    "                         scoring='neg_mean_squared_error', cv=5,\n",
    "                         subsample=1.0, n_jobs=1,\n",
    "                         max_time_mins=None, max_eval_time_mins=5,\n",
    "                         random_state=None, config_dict=None,\n",
    "                         warm_start=False,\n",
    "                         memory=None,\n",
    "                         use_dask=False,\n",
    "                         periodic_checkpoint_folder=None,\n",
    "                         early_stop=None,\n",
    "                         verbosity=0,\n",
    "                         disable_update_check=False)\n",
    "        tpot.fit(X_train, y_train)\n",
    "        print(tpot.score(X_test, y_test))\n",
    "        tpot.export('tpot_demand_pipeline.py')\n",
    "\n",
    "TrainingObj = DemandTrain()\n",
    "TrainingObj.trainRF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/apps/Jaarvis_Demand_Supply'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=None)\n",
    "\n",
    "# Average CV score on the training set was:-113.19816539397762\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=ElasticNetCV(l1_ratio=0.75, tol=0.01)),\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.99, learning_rate=0.01, loss=\"lad\", max_depth=6, max_features=0.7500000000000001, min_samples_leaf=18, min_samples_split=13, n_estimators=100, subsample=0.05)),\n",
    "    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n",
    "    RandomForestRegressor(bootstrap=True, max_features=0.8500000000000001, min_samples_leaf=18, min_samples_split=11, n_estimators=100)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
